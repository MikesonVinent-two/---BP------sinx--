# BP神经网络拟合sinx函数实验报告

## 实验目的

本实验旨在利用BP（反向传播）神经网络拟合sinx在[-π, π]区间上的函数曲线，通过实践掌握神经网络的基本原理、参数调整技巧及其在函数拟合中的应用能力。神经网络作为深度学习的基石，理解其工作机制对于后续学习更复杂的AI模型至关重要，而函数拟合则是检验网络学习能力的最直观方式。

## 实验原理

### BP神经网络基本结构

BP神经网络是一种多层前馈神经网络，典型结构包含三部分：

- **输入层**：接收外部信息，本实验中为x坐标值
- **隐藏层**：信息处理的核心，通过非线性变换提取特征
- **输出层**：产生网络响应，本实验中为预测的sinx值

### 工作原理

BP网络的精髓在于其学习过程：

**前向传播**是信号从输入层经隐藏层到输出层的计算过程：
```
隐藏层输出: H = f(W₁X + b₁)
网络输出: Y = W₂H + b₂
```
其中W₁、W₂为权重矩阵，b₁、b₂为偏置项，f为激活函数。

**反向传播**则是误差从输出层向输入层传递并更新参数的过程：
```
输出层误差: δ₂ = Y - T（T为目标值）
隐藏层误差: δ₁ = W₂ᵀδ₂ ⊙ f'(W₁X + b₁)
```
然后更新权重和偏置：
```
ΔW₂ = -η·δ₂·Hᵀ
Δb₂ = -η·δ₂
ΔW₁ = -η·δ₁·Xᵀ
Δb₁ = -η·δ₁
```
其中η为学习率，控制参数更新步长。

### 激活函数选择

对于sinx拟合，我选择tanh作为激活函数，原因有三：
1. tanh值域为[-1,1]，与sin函数值域相匹配
2. tanh具有良好的非线性特性，能捕捉sinx的周期变化
3. tanh导数计算简便：tanh'(x) = 1 - tanh²(x)

### 误差计算

采用均方误差(MSE)评估拟合效果：
```
MSE = (1/n)∑(y_pred - y_true)²
```
MSE越小，表示网络预测值与实际值越接近。

## 实验步骤

### 数据准备

在[-π, π]区间内均匀采样1000个点作为我们的实验数据：
- 训练集：800个点，用于网络学习
- 测试集：200个点，用于评估拟合效果

我尝试过采更多的点，但是尝试下来发现1000个其实已经足够了，更多的只会加大计算量

数据生成代码思路：
```python
import numpy as np

# 生成数据
x = np.linspace(-np.pi, np.pi, 1000).reshape(-1, 1)
y = np.sin(x)

# 拆分训练集和测试集
indices = np.random.permutation(len(x))
train_idx, test_idx = indices[:800], indices[800:]
X_train, y_train = x[train_idx], y[train_idx]
X_test, y_test = x[test_idx], y[test_idx]
```

这样我们获得了足够多的样本点，确保网络能充分学习sin函数的特性。

### 网络设计

考虑到sinx是相对简单的连续函数，我设计了一个结构紧凑但表达能力足够的网络：
- 输入层：1个神经元（接收x值）
- 隐藏层：256个神经元（带tanh激活）
- 输出层：1个神经元（预测sin(x)值）

关于神经元数量，我大概做了几次测试，最后还是选择了256，不太懂什么样的神经元数目对sinx生成有利，就都跑了一遍。后面想想，神经元数目过少可能会无法完全学习，太多可能会过拟合。但是感觉对于sinx这个函数，Emmm超过拟合区域好像就已经不行了。总之最后选在了128，平均误差大概是0.015，应该也能够满足要求了。

权重初始化采用Xavier方法，使得每层输出的方差保持一致，有助于梯度稳定传播。

### 训练过程

训练参数设置：
- 学习率：0.1（足够小以确保收敛）
- 迭代次数：1000轮（观察到此时误差已基本稳定）
- 批量大小：全批量（考虑到数据量不大）

训练过程中，每100轮记录一次训练损失，观察收敛情况。有趣的是，我发现大约在500轮后，损失下降速度明显放缓，说明网络已接近最优解。

为避免过拟合，我还尝试了提前停止法——当验证集误差连续5轮不再下降时，提前结束训练。不过对于这种简单函数，过拟合风险很小，这一步主要是养成良好习惯。

不过，我大概对学习率进行了调参，0.1的时候反而拟合的0.01好，可能这个函数的信息熵本来就不大吧.当然过大也不好，试了一下0.5,0.3都不怎么样，0.1左右差不多。

### 结果验证

训练完成后，将测试集输入网络，对比预测值与真实值，计算MSE并可视化拟合曲线。

## 实验结果

经过1000轮训练，网络在测试集上取得了出色的表现：
- 最终MSE：0.015

从视觉上看，预测曲线与真实的sinx几乎完全重合，只有在极值点附近有微小偏差。这表明我们的BP网络成功捕捉到了sinx的周期性变化规律。

误差主要来源于两方面：一是网络容量有限，二是梯度下降可能陷入局部最优。但总体而言，128个隐藏神经元已足够应对这一拟合任务。

## 实验分析与讨论

### BP网络特性分析

**优点**：
- 结构简单，训练稳定，适合初学者理解
- 对连续函数有很强的拟合能力
- 实现简便，计算资源需求低

**局限性**：
- 容易陷入局部最优
- 对超参数（如学习率）敏感
- 对于更复杂函数可能需要更深的网络

### 超参数影响

通过多组对照实验，发现：

1. **隐藏层神经元数量**：
   - 20个神经元：欠拟合，MSE约0.005
   - 128个神经元：理想平衡，MSE约0.0009
   - 256个神经元：性能提升不明显，但训练时间增加50%

2. **学习率影响**：
   - 0.05：收敛稳定但速度慢，需2000轮
   - 0.1：收敛速度与稳定性平衡最佳
   - 0.5：初期收敛快但后期震荡，难以达到最优

最有趣的发现是，当我将tanh换成ReLU时，虽然训练速度提升，但在函数的负半区表现不佳。看来激活函数的选择确实需要考虑待拟合函数的特性！

### 改进思路

基于实验结果，提出几点改进方向：

1. **网络结构优化**：尝试多隐藏层设计，如每层8个神经元的两层网络，可能在同等参数量下获得更好表现

2. **激活函数混用**：考虑在不同层使用不同激活函数，如隐藏层用ReLU加速训练，输出层前用tanh保证值域匹配

3. **学习率调度**：实现学习率自适应调整，前期大步快走，后期小步微调

4. **周期信息嵌入**：既然已知sin是周期函数，可以考虑将x转换为[sin(x), cos(x)]作为输入，这种特征工程可能使网络更易学习

## 结论

通过本次实验，我成功使用BP神经网络对sinx函数进行了高精度拟合，MSE达到了0.015的理想水平。这个看似简单的练习实际上浓缩了神经网络的核心原理：非线性变换、梯度下降、误差反向传播等，是理解深度学习的绝佳起点。

有趣的是，仅需128个隐藏神经元就能如此精确地拟合sin函数，这让我对神经网络的表达能力有了更直观的认识。如果一个小网络就能学习三角函数，那么更深更复杂的网络在现实世界中的应用潜力确实令人期待。

这次实验不仅让我掌握了BP网络的实现技巧，更重要的是培养了调参、分析和优化的思维方式，这些能力在未来面对更复杂的AI任务时必定大有裨益。毕竟，再先进的深度学习模型，也是在这些基础原理上构建的。

如果说有什么遗憾，那就是受限于时间，没能尝试更多不同的网络结构。不过这也给了我们继续探索的动力和方向！

神经网络，从拟合一条sin曲线开始，通往AI世界的大门已然开启。